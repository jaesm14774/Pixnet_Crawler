{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "category_list -> article_url -> pixnet_crawler 每天收集資料(熱門貼文)\n",
    "\n",
    "假如known author uuid -> article_url_by_author -> pixnet_crawler 收集特定喜歡的作者貼文\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = [\n",
    "\"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
    "\"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
    "\"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0\",\n",
    "\"Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; .NET4.0C; .NET4.0E; .NET CLR 2.0.50727; .NET CLR 3.0.30729; .NET CLR 3.5.30729; InfoPath.3; rv:11.0) like Gecko\",\n",
    "\"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\",\n",
    "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\n",
    "\"Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\n",
    "\"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11\",\n",
    "\"Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11\",\n",
    "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Avant Browser)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\"\n",
    "]\n",
    "\n",
    "header={'User-Agent':random.choice(user_agent)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取得 痞客幫 category list\n",
    "def get_pixnet_list():\n",
    "    a=requests.get('https://www.pixnet.net/blog?utm_source=PIXNET&utm_medium=navbar&utm_term=blog',\n",
    "                  headers=header,timeout=20)\n",
    "    a.encoding='utf8'\n",
    "\n",
    "    soup=BeautifulSoup(a.text,'lxml')\n",
    "\n",
    "    C=[[s.find('a').get('href'),s.find('a').text.strip()] for s in soup.find('ul',{'id':'navigation'}).find_all('li')]\n",
    "    C=[s for s in C if s[0]!= '/blog']\n",
    "    \n",
    "    C=pd.DataFrame(C,columns=['url','name'])\n",
    "    \n",
    "    C['url']='https://www.pixnet.net'+C['url']\n",
    "    \n",
    "    return C\n",
    "\n",
    "C=get_pixnet_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#痞客幫內文、留言\n",
    "def pixnet_crawler(url,comment_bool=True):\n",
    "    a=requests.get(url,headers=header,timeout=20)\n",
    "    a.encoding='utf8'\n",
    "\n",
    "    soup=BeautifulSoup(a.text,'lxml')\n",
    "\n",
    "    #收集內文\n",
    "    \n",
    "    #id\n",
    "    try:\n",
    "        #多數人為這個形式\n",
    "        id_part1=re.search(string=url,pattern='/([A-z0-9]+)\\.pixnet\\.net').group(1)\n",
    "    except:\n",
    "        id_part1=re.search(string=url,pattern='/([\\.A-z0-9-]+)/blog').group(1)\n",
    "\n",
    "    id_part2=re.search(string=url,pattern='post/(\\d+)').group(1)\n",
    "\n",
    "    article_id=id_part1+'-'+id_part2\n",
    "\n",
    "    article_url=f'https://{id_part1}.pixnet.net/blog/post/{id_part2}'\n",
    "\n",
    "    print(article_url)\n",
    "\n",
    "    #作者唯一id、不知名類別id、分享數、留言數、留言總頁數\n",
    "    info_block=soup.find('body',class_=re.compile(f'article_{id_part2}'))\n",
    "\n",
    "    author_pixnet_id=info_block.get('data-author-member-uniqid')\n",
    "    author_id=id_part1\n",
    "    \n",
    "    share_count=info_block.get('data-shareds-count')\n",
    "\n",
    "    comment_count=info_block.get('data-comments-count')\n",
    "    \n",
    "    comment_closed_page=int(np.ceil((int(comment_count)/100)))\n",
    "    #發文時間\n",
    "    created_at=soup.find('ul',class_='article-head').find(class_='publish').get('content')\n",
    "    created_at=created_at[:created_at.find('.')]\n",
    "    created_at=datetime.datetime.strptime(created_at,'%Y-%m-%dT%H:%M:%S').strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    #標題\n",
    "    title=soup.find('ul',class_='article-head').find('h2').text.strip()\n",
    "\n",
    "    #推文、收藏數\n",
    "    try:\n",
    "        collect_count=requests.get(f'https://{id_part1}.pixnet.net/api/blog/collectionstatus?object_id=https%3A%2F%2F{id_part1}.pixnet.net%2Fblog%2Fpost%2F{id_part2}&type=article',\n",
    "                     headers=header,timeout=20).json()['data']['collect_count']\n",
    "    except:\n",
    "        collect_count=np.nan\n",
    "    \n",
    "    try:\n",
    "        recommend_count=requests.get(f'https://{id_part1}.pixnet.net/api/blog/articlepushcounts?article_id={id_part2}',\n",
    "                     headers=header,timeout=20).json()['data']['total_recommend_count']\n",
    "    except:\n",
    "        recommend_count=np.nan\n",
    "        \n",
    "    #FB按讚數\n",
    "    temp=requests.get(f'https://www.facebook.com/plugins/like.php?href=http%3A%2F%2F{id_part1}.pixnet.net%2Fblog%2Fpost%2F{id_part2}&layout=button_count&show_faces=false&width=90&action=like&font=arial&colorscheme=light&height=21',\n",
    "                     headers={'User-Agent':random.choice(user_agent),\n",
    "                              'Cookie':'c_user=100002615484088; xs=15%3AEAgYmtuLadf4nQ%3A2%3A1632559371%3A-1%3A11327%3A%3AAcX6sKEQKPcuYVnojBTDpPNfZCbPby0erjk_biX8FQ; fr=0pSi1ugyZpgDmAb4E.AWXXRQ3rBxIZj3817qRuMOcEtgs.BhXZkM.L_.AAA.0.0.BhXZkM.AWWLSL6sXYA; spin=r.1004530306_b.trunk_t.1633959989_s.1_v.2_',\n",
    "                             },timeout=20)\n",
    "\n",
    "    like_count=BeautifulSoup(temp.text,'lxml').find(class_='_5n6h _2pih').text\n",
    "    \n",
    "#     like_count=np.nan\n",
    "\n",
    "    #作者、標籤、內文、table數、image數、影片數(youtube)\n",
    "    all_table=soup.find('div',class_='article-body').find_all('table')\n",
    "    table_count=len(all_table)\n",
    "\n",
    "    img=[s for s in soup.find('div',class_='article-body').find_all(re.compile('img|picture')) if s.get('title') is not None and s.get('title') != '']\n",
    "    img=[s.get('src') for s in img]\n",
    "    \n",
    "    all_img=[]\n",
    "    \n",
    "    for s in img:\n",
    "        if bool(re.search(string=s,pattern='^https')):\n",
    "            all_img.append(s)\n",
    "        else:\n",
    "            all_img.append('https:'+s)\n",
    "    \n",
    "    img_count=len(all_img)\n",
    "\n",
    "    all_frame=soup.find('div',class_='article-body').find_all('iframe')\n",
    "    video_count=len([s for s in all_frame if bool(re.search(string=str(s),\n",
    "                                                            pattern='autoplay|youtube\\.com'))])\n",
    "\n",
    "    author=soup.find(class_='author-profile__name').text.strip()\n",
    "\n",
    "    tag=';'.join([s.text.strip() for s in soup.find('div',class_='article-body').find_all(class_='tag')])\n",
    "\n",
    "    #完整版內文(+表格、圖片、影片描述文字)\n",
    "    temp=soup.find('div',class_='article-body')\n",
    "    temp=str(temp)\n",
    "    temp=temp[:temp.find('<div class=\"tag-container-parent\">')]\n",
    "\n",
    "    temp=BeautifulSoup(temp,'lxml')\n",
    "    content_origin=temp.text.strip()\n",
    "\n",
    "    #去除版內文\n",
    "    for s in temp.find_all(re.compile('iframe|img|table')):\n",
    "        s.decompose()\n",
    "\n",
    "    content=temp.text.strip()\n",
    "    \n",
    "    #文章分類、個人分類\n",
    "    temp=soup.find('ul',class_='refer').find_all('li')\n",
    "\n",
    "    category=[s.text.split('：')[1] for s in temp if bool(re.search(string=str(s),pattern='全站分類'))][0]\n",
    "\n",
    "    board=[s.find('a').get('href') for s in temp if bool(re.search(string=str(s),pattern='全站分類'))][0]\n",
    "    board=re.search(string=board,pattern='articles/(.*)').group(1)\n",
    "    \n",
    "    self_category=[s.text.split('：')[1] for s in temp if bool(re.search(string=str(s),pattern='個人分類'))]\n",
    "    \n",
    "    if len(self_category) >0:\n",
    "        self_category=self_category[0]\n",
    "    else:\n",
    "        self_category=''\n",
    "    \n",
    "    post=pd.DataFrame({\n",
    "            'board_id':board,\n",
    "            'article_id':article_id,\n",
    "            'author_pixnet_id':author_pixnet_id,\n",
    "            'author_id':author_id,\n",
    "            'author':author,\n",
    "            'created_at':created_at,\n",
    "            'title':title,\n",
    "            'content':content,\n",
    "            'content_origin':content_origin,\n",
    "            'category':category,\n",
    "            'self_category':self_category,\n",
    "            'collect_count':collect_count,\n",
    "            'recommend_count':recommend_count,\n",
    "            'comment_count':comment_count,\n",
    "            'share_count':share_count,\n",
    "            'like_count':like_count,\n",
    "            'table_count':table_count,\n",
    "            'image_count':img_count,\n",
    "            'video_count':video_count,\n",
    "            'tag':tag,\n",
    "            'image_url':';'.join(all_img),\n",
    "            'article_url':article_url,\n",
    "    },index=[0])\n",
    "    \n",
    "    #收集留言\n",
    "    C=[]\n",
    "    \n",
    "    cn=0\n",
    "    \n",
    "    for p in range(1,comment_closed_page+1):\n",
    "        comment_url=f'https://{id_part1}.pixnet.net/blog/post/{id_part2}/comments?comment_page={p}'\n",
    "\n",
    "        comment=requests.get(comment_url,\n",
    "                             headers=header,timeout=20).json()\n",
    "\n",
    "        temp=BeautifulSoup(comment['list'].strip(),'lxml')\n",
    "\n",
    "        for c in temp.find_all(class_='single-post'):\n",
    "            if 'secret' in c.get('class'):\n",
    "                #自創悄悄話的unique_id\n",
    "                comment_id=f'{article_id}-secret-{cn}'\n",
    "                cn=cn+1\n",
    "                \n",
    "                author=''\n",
    "                author_id=''\n",
    "                author_img=''\n",
    "                floor=''\n",
    "                created_at=None\n",
    "                content=c.find(class_='post-text').text.strip()\n",
    "                reply_author=''\n",
    "                reply_content=''\n",
    "                reply_author_id=''\n",
    "                reply_time=None\n",
    "            else:\n",
    "                comment_id=c.find(class_='post-info').find('a').get('name')\n",
    "                author=c.find(class_='post-info').find(class_='user-name').text.strip()\n",
    "\n",
    "                #訪客或是帳號為有連結(已被刪除?)\n",
    "                try:\n",
    "                    author_id=c.find(class_='post-info').find(class_='user-name').find('a').get('href')\n",
    "                    author_id=re.search(string=author_id,pattern='/pcard/([A-z0-9]+)$').group(1)\n",
    "                except:\n",
    "                    #unknown\n",
    "                    author_id=''\n",
    "\n",
    "                floor=c.find(class_='post-info').find(class_='floor').text.strip('#')\n",
    "\n",
    "                created_at=c.find(class_='post-info').find(class_='post-time').text.strip()\n",
    "                created_at=re.search(string=created_at,pattern='\\d+/\\d+/\\d+ \\d+:\\d+').group(0)\n",
    "                created_at=datetime.datetime.strptime(created_at,'%Y/%m/%d %H:%M').strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                author_img=c.find(class_='post-photo').find('img').get('src')\n",
    "\n",
    "                content=c.find(class_='post-text').text.strip()\n",
    "\n",
    "                if c.find(class_='reply-text') is None:\n",
    "                    reply_author=''\n",
    "                    reply_content=''\n",
    "                    reply_author_id=''\n",
    "                    reply_time=None\n",
    "                else:\n",
    "                    reply_author=c.find(class_='reply-text').find('a').text.strip()\n",
    "\n",
    "                    u=c.find(class_='reply-text').find('p').find('a').get('href')\n",
    "                    try:\n",
    "                        #多數人為這個形式\n",
    "                        reply_author_id=re.search(string=u,pattern='/([A-z0-9]+)\\.pixnet\\.net').group(1)\n",
    "                    except:\n",
    "                        reply_author_id=re.search(string=u,pattern='/([\\.A-z0-9-]+)/blog').group(1)\n",
    "\n",
    "                    t=c.find(class_='reply-text').text.strip()\n",
    "\n",
    "                    reply_content=t.split(f'{reply_author} 於')[0].strip()\n",
    "\n",
    "                    reply_time=re.search(string=t,pattern='\\d+/\\d+/\\d+ \\d+:\\d+').group(0)\n",
    "                    reply_time=datetime.datetime.strptime(reply_time,'%Y/%m/%d %H:%M').strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            C.append(pd.DataFrame({\n",
    "                'board_id':board,\n",
    "                'article_id':article_id,\n",
    "                'comment_id':comment_id,\n",
    "                'author_id':author_id,\n",
    "                'author':author,\n",
    "                'author_img':author_img,\n",
    "                'floor':floor,\n",
    "                'created_at':created_at,\n",
    "                'content':content,\n",
    "                'reply_author_id':reply_author_id,\n",
    "                'reply_author':reply_author,\n",
    "                'reply_time':reply_time,\n",
    "                'reply_content':reply_content,\n",
    "                'article_url':article_url\n",
    "            },index=[0]))    \n",
    "    \n",
    "    if len(C)>0:\n",
    "        C=pd.concat(C).reset_index(drop=True)\n",
    "    else:\n",
    "        C=pd.DataFrame()\n",
    "    \n",
    "    if comment_bool:\n",
    "        return post,C\n",
    "    else:\n",
    "        return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抓取一般首頁文章網址(用來找熱門作者)\n",
    "def get_article_url(url,page):\n",
    "    au=[]\n",
    "    \n",
    "    for p in range(1,page+1):\n",
    "        a=requests.get(f'{url}/latest/{p}',headers=header,timeout=20)\n",
    "        a.encoding='utf8'\n",
    "        \n",
    "        soup=BeautifulSoup(a.text,'lxml')\n",
    "        \n",
    "        target=[s.get('href') for s in soup.find(class_='box-body').find_all('a') if s.get('href').find('blog/post') != -1]\n",
    "\n",
    "        au.extend(target)\n",
    "\n",
    "    return au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_u=[]\n",
    "\n",
    "page=5\n",
    "\n",
    "for u in C['url']:\n",
    "    all_u.extend(get_article_url(url=u,page=page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(url):\n",
    "    #id\n",
    "    try:\n",
    "        #多數人為這個形式\n",
    "        id_part1=re.search(string=url,pattern='/([A-z0-9]+)\\.pixnet\\.net').group(1)\n",
    "    except:\n",
    "        id_part1=re.search(string=url,pattern='/([\\.A-z0-9-]+)/blog').group(1)\n",
    "\n",
    "    id_part2=re.search(string=url,pattern='post/(\\d+)').group(1)\n",
    "\n",
    "    article_id=id_part1+'-'+id_part2\n",
    "\n",
    "    article_url=f'https://{id_part1}.pixnet.net/blog/post/{id_part2}'\n",
    "    \n",
    "    return article_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_u=[clean_url(s) for s in all_u]\n",
    "\n",
    "all_u=np.unique(all_u).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R=[]\n",
    "Co=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in all_u[0:]:\n",
    "    try:\n",
    "        p,c=pixnet_crawler(url=u,comment_bool=True)\n",
    "\n",
    "        R.append(p)\n",
    "        Co.append(c)\n",
    "    except Exception as e:\n",
    "        print(f'Error for : {u}')\n",
    "        print(e,'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat(R)\n",
    "df=df[~df.duplicated(subset=['article_id'])]\n",
    "df=df.sort_values(['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Co=pd.concat(Co)\n",
    "Co=Co[~Co.duplicated(subset=['article_id','comment_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download all image\n",
    "def download_media(path,df):\n",
    "    df=df[~df.duplicated(subset=['article_id'])]\n",
    "    df=df.where(~df.isnull(),None)\n",
    "    df=df.reset_index(drop=True)\n",
    "\n",
    "    for i in range(0,df.shape[0]):\n",
    "        uid=df.loc[i,'article_id']\n",
    "        author=df.loc[i,'author']+'-'+df.loc[i,'author_pixnet_id']\n",
    "        \n",
    "        #edge_sidecar_to_children\n",
    "        if df.loc[i,'image_url'] is None or df.loc[i,'image_url'] == '':\n",
    "            target=[]\n",
    "        else:\n",
    "            target=df.loc[i,'image_url']\n",
    "            target=target.split(';')\n",
    "\n",
    "        image_target=target\n",
    "        \n",
    "        if len(image_target)>0:\n",
    "            try:\n",
    "                if not os.path.isdir(f'{path}{author}/'):\n",
    "                    os.makedirs(f'{path}{author}/')\n",
    "            except:\n",
    "                #名子不符合磁碟要求命名規則\n",
    "                author=df.loc[i,'author_pixnet_id']\n",
    "                if not os.path.isdir(f'{path}{author}/'):\n",
    "                    os.makedirs(f'{path}{author}/')                \n",
    "        \n",
    "        for i,j in enumerate(image_target):\n",
    "            try:\n",
    "                a=requests.get(j,headers=header,timeout=20)\n",
    "\n",
    "                with open(path+f'{author}/{uid}_{i}.jpg','wb') as f:\n",
    "                    f.write(a.content)\n",
    "            except Exception as e:\n",
    "                print(f'Error for url : {j}')\n",
    "                print(e,'\\n\\n')\n",
    "\n",
    "#download media\n",
    "if len(df)>0:\n",
    "    path='/self_project/self_django/self_keep/pixnet/'\n",
    "\n",
    "    download_media(path=path,df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#由最新文章開始往回取幾頁，如果為None，代表爬所有文章網址\n",
    "def get_article_url_by_author(author_pixnet_id,page):\n",
    "    u1=f'https://www.pixnet.net/pcard/api/publish/articles?owner_uniq_id={author_pixnet_id}&per_page=10&type=0'\n",
    "    \n",
    "    n=1\n",
    "    \n",
    "    if page is None:\n",
    "        page=9999\n",
    "    \n",
    "    deter=0\n",
    "    \n",
    "    U=[]\n",
    "    \n",
    "    while deter == 0 and n<=page:\n",
    "        if n == 1:\n",
    "            a=requests.get(u1,headers=header,timeout=20).json()\n",
    "        else:\n",
    "            u=f'https://www.pixnet.net/pcard/api/publish/articles?owner_uniq_id={author_pixnet_id}&per_page=10&type=0&page_info={page_info}'\n",
    "\n",
    "            a=requests.get(u1,headers=header,timeout=20).json()\n",
    "        \n",
    "        if 'page_info' in a['data'].keys():\n",
    "            page_info=a['data']['page_info']\n",
    "        else:\n",
    "            deter=1\n",
    "        \n",
    "        U.append(pd.DataFrame(a['data']['articles']).drop(['images'],axis=1))\n",
    "        \n",
    "        n=n+1\n",
    "    \n",
    "    if len(U)>0:\n",
    "        return pd.concat(U).reset_index(drop=True)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加上爬蟲時間\n",
    "df['create_time']=format(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "C['create_time']=format(datetime.datetime.now()-datetime.timedelta(hours=8),'%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "import MySQLdb\n",
    "\n",
    "path='/additional_file/sql_config.txt'\n",
    "\n",
    "connection=pd.read_csv(path,index_col='name')\n",
    "\n",
    "def get_sql(host,port,user,password,db_name):\n",
    "    \"\"\"\n",
    "    host:server ip of sql\n",
    "    port:port sql\n",
    "    user:user name of sql\n",
    "    password:password of sql user\n",
    "    db_name:name of database\n",
    "    \"\"\"\n",
    "    conn=pymysql.connect(host=host,\n",
    "                         port=int(port),\n",
    "                         user=user,\n",
    "                         password=password,\n",
    "                         db=db_name)\n",
    "    cursor=conn.cursor()\n",
    "    engine = create_engine('mysql+mysqldb://%s:%s@%s:%s/%s?charset=utf8mb4' % \n",
    "                           (user,password,host,port,db_name))\n",
    "    return (conn,cursor,engine)\n",
    "\n",
    "conn,cursor,engine=get_sql(connection.loc['ip','value'],int(connection.loc['port','value']),\n",
    "                           connection.loc['user','value'],\n",
    "                           connection.loc['password','value'],'crawler')       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
